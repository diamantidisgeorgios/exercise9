{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPEu2m310lNGNdvjZQuFwx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f27cbfc72b964bcf87f6b68265954d8d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_302c0881e68c44988a42884ad1284284",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  91%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m18,224/20,000 \u001b[0m [ \u001b[33m0:00:42\u001b[0m < \u001b[36m0:00:05\u001b[0m , \u001b[31m403 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  91%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">18,224/20,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:42</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:05</span> , <span style=\"color: #800000; text-decoration-color: #800000\">403 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "302c0881e68c44988a42884ad1284284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diamantidisgeorgios/exercise9/blob/main/exercise_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install libraries"
      ],
      "metadata": {
        "id": "0BmE4iIkUsjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "cb4eVJfKUVTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93ae20b-690e-4e5d-9233-1c259dec78ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zijCPAlUOux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1011d6be-3054-4376-9fc0-91cffd5c0f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d"
      ],
      "metadata": {
        "id": "zZictiIKUnK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a03eecb-0d37-4fef-8571-9ac72aebe3c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: box2d\n",
            "Successfully installed box2d-2.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install renderlab"
      ],
      "metadata": {
        "id": "rZZhmxE9UqO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a0e402-c5ae-4ff1-b87a-1916f94a58a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting renderlab\n",
            "  Downloading renderlab-0.1.20230421184216-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.0.3)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (0.0.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2026.1.4)\n",
            "Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: renderlab\n",
            "Successfully installed renderlab-0.1.20230421184216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic functions\n",
        "\n",
        "Every gymnasium environment has 3 basic functions: step, reset, and render.\n",
        "Step: updates an environment with actions.\n",
        "Reset: Resets the environment to an initial state.\n",
        "Render: Renders the environment."
      ],
      "metadata": {
        "id": "n3XEeB5SVIQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LunarLander-v3 enviroment\n",
        "##-Observation space\n",
        "It is an 8-dimensional vector with information about the current state of the lunar lander. It has 1.The coordinates of x, 2.The coordinates of y, 3.The linear velocity of x, 4.The linear velocity of y, 5.its angle, 6. its angular velocity, 7.Boolean that represents if a the left leg is in contact with the ground or not, 8.Boolean that represents if a the right leg is in contact with the ground or not\n",
        "##-Action space\n",
        "It tells the lunar lander what to do. It has four discrete actions. 1.Do nothing, 2.Fire left orientation engine, 3.Fire main engine,4.Fire right orientation engine\n",
        "##-Reward Function\n",
        "After every step a reward is given."
      ],
      "metadata": {
        "id": "JGYbFsJ4N5vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a random agent and visualize its gameplay"
      ],
      "metadata": {
        "id": "-oy6ylu8QETr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import renderlab as rl\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "#env = rl.RenderFrame(env, './output')\n",
        "\n",
        "osbservation,info = env.reset()\n",
        "while True:\n",
        "  action = env.action_space.sample()\n",
        "  osbservation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  if terminated or truncated:\n",
        "    break\n",
        "\n",
        "#env.play()\n"
      ],
      "metadata": {
        "id": "yWGW2jzAVTyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the random agent 5 times and get its mean reward"
      ],
      "metadata": {
        "id": "F-rF5k6EQc_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "rewards = []\n",
        "osbservation,info = env.reset()\n",
        "k=5\n",
        "for i in range(k):\n",
        "  action = env.action_space.sample()\n",
        "  osbservation, reward, terminated, truncated, info = env.step(action)\n",
        "  rewards.append(reward)\n",
        "  print(f\"Reward for episode {i}: {reward}\")\n",
        "  if terminated or truncated:\n",
        "    continue\n",
        "\n",
        "print(f\"Mean reward: {np.mean(rewards)}\")\n"
      ],
      "metadata": {
        "id": "1ty192o2ZR84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a223aad-3cb6-40f2-ede5-6514ddca9014"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward for episode 0: 0.39796232130720566\n",
            "Reward for episode 1: -4.272129060042192\n",
            "Reward for episode 2: -0.35764476751000984\n",
            "Reward for episode 3: -1.4438996061205944\n",
            "Reward for episode 4: -3.663771420254551\n",
            "Mean reward: -1.8678965065240285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install stable-baselines3"
      ],
      "metadata": {
        "id": "xf0IoYgKQobO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "toV9jXvUasFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use the DQN algorithm"
      ],
      "metadata": {
        "id": "BtEHP5cpQ0Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "import time\n",
        "\n",
        "dqn_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "dqn_model = DQN(\"MlpPolicy\", dqn_env, verbose=1)\n",
        "dqn_model.learn(total_timesteps=20000, log_interval=4, progress_bar=True)\n",
        "dqn_time = time.time() - start\n",
        "\n"
      ],
      "metadata": {
        "id": "g0VdpL3TagoW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f27cbfc72b964bcf87f6b68265954d8d",
            "302c0881e68c44988a42884ad1284284"
          ]
        },
        "outputId": "7e7b84dc-d5ca-4e29-ce2d-03f5a319ab29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f27cbfc72b964bcf87f6b68265954d8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86       |\n",
            "|    ep_rew_mean      | -265     |\n",
            "|    exploration_rate | 0.837    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 541      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 344      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.67     |\n",
            "|    n_updates        | 60       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.2     |\n",
            "|    ep_rew_mean      | -223     |\n",
            "|    exploration_rate | 0.661    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 434      |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total_timesteps  | 714      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.89     |\n",
            "|    n_updates        | 153      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 113      |\n",
            "|    ep_rew_mean      | -275     |\n",
            "|    exploration_rate | 0.355    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 472      |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total_timesteps  | 1357     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.34     |\n",
            "|    n_updates        | 314      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 124      |\n",
            "|    ep_rew_mean      | -355     |\n",
            "|    exploration_rate | 0.0576   |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 480      |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 1984     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.21     |\n",
            "|    n_updates        | 470      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 116      |\n",
            "|    ep_rew_mean      | -365     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 479      |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total_timesteps  | 2327     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.16     |\n",
            "|    n_updates        | 556      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -316     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 502      |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total_timesteps  | 2746     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.58     |\n",
            "|    n_updates        | 661      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 118      |\n",
            "|    ep_rew_mean      | -283     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 523      |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total_timesteps  | 3298     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.19     |\n",
            "|    n_updates        | 799      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 121      |\n",
            "|    ep_rew_mean      | -285     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 540      |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 3860     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 939      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 125      |\n",
            "|    ep_rew_mean      | -280     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 522      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 4511     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.649    |\n",
            "|    n_updates        | 1102     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -272     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 520      |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total_timesteps  | 5150     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 1262     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 141      |\n",
            "|    ep_rew_mean      | -282     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 488      |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 6186     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 1521     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 145      |\n",
            "|    ep_rew_mean      | -281     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 479      |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 6966     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.841    |\n",
            "|    n_updates        | 1716     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 147      |\n",
            "|    ep_rew_mean      | -278     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 484      |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 7641     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.54     |\n",
            "|    n_updates        | 1885     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 158      |\n",
            "|    ep_rew_mean      | -275     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 485      |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total_timesteps  | 8833     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.976    |\n",
            "|    n_updates        | 2183     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 160      |\n",
            "|    ep_rew_mean      | -268     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 486      |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total_timesteps  | 9622     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.602    |\n",
            "|    n_updates        | 2380     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 166      |\n",
            "|    ep_rew_mean      | -265     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 496      |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 10605    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.801    |\n",
            "|    n_updates        | 2626     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 193      |\n",
            "|    ep_rew_mean      | -264     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 463      |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 13097    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.673    |\n",
            "|    n_updates        | 3249     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 216      |\n",
            "|    ep_rew_mean      | -276     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 451      |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 15568    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.691    |\n",
            "|    n_updates        | 3866     |\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = dqn_env.reset()\n",
        "    while True:\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, dqn_reward, terminated, truncated, info = dqn_env.step(action)\n",
        "      total_reward += dqn_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        dqn_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(dqn_rewards)}\")\n"
      ],
      "metadata": {
        "id": "qdCcglqob4E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use the PPO algorithm"
      ],
      "metadata": {
        "id": "H-a0x8ezSZg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "ppo_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "ppo_model = PPO(\"MlpPolicy\", ppo_env, verbose=1)\n",
        "ppo_model.learn(total_timesteps=20000, progress_bar=True)\n",
        "ppo_time = time.time() - start"
      ],
      "metadata": {
        "id": "VS61v7FBwkiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = ppo_env.reset()\n",
        "    while True:\n",
        "      action, _states = ppo_model.predict(obs)\n",
        "      obs, ppo_reward, terminated, truncated, info = ppo_env.step(action)\n",
        "      total_reward += ppo_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        ppo_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(ppo_rewards)}\")"
      ],
      "metadata": {
        "id": "KFQwibryoP-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot the time it took to train for each algorithm"
      ],
      "metadata": {
        "id": "bKNqaXd7S2EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar([\"DQN\", \"PPO\"], [dqn_time, ppo_time])\n",
        "plt.title(\"Training time\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3CMOWK5kTJMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot rewards per episode for each algorithm"
      ],
      "metadata": {
        "id": "ecoa2GeBTsU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.title(\"Rewards per episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_AugCYvTUhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redoing the previous steps but with a different parameters for the algorithms"
      ],
      "metadata": {
        "id": "yVxEOuJnT-i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dqn_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "dqn_model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    dqn_env,\n",
        "    buffer_size=200_000,\n",
        "    learning_starts=10_000,\n",
        "    batch_size=128,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    target_update_interval=1_000,\n",
        ")\n",
        "dqn_model.learn(total_timesteps=500000, log_interval=400, progress_bar=True)\n",
        "dqn_time = time.time() - start\n",
        "\n",
        "dqn_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = dqn_env.reset()\n",
        "    while True:\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, dqn_reward, terminated, truncated, info = dqn_env.step(action)\n",
        "      total_reward += dqn_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        dqn_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(dqn_rewards)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kFnaJ7BgUE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "ppo_model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    ppo_env,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    gamma=0.99,\n",
        ")\n",
        "ppo_model.learn(total_timesteps=200000, progress_bar=True)\n",
        "ppo_time = time.time() - start\n",
        "\n",
        "ppo_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = ppo_env.reset()\n",
        "    while True:\n",
        "      action, _states = ppo_model.predict(obs)\n",
        "      obs, ppo_reward, terminated, truncated, info = ppo_env.step(action)\n",
        "      total_reward += ppo_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        ppo_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(ppo_rewards)}\")"
      ],
      "metadata": {
        "id": "_sp4VRB7UJOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar([\"DQN\", \"PPO\"], [dqn_time, ppo_time])\n",
        "plt.title(\"Training time\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yeST8TLZUUlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.title(\"Rewards per episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dAeCIMPkUWDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}