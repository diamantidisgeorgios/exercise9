{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMzwUdlh4oFfl54duC0/pT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diamantidisgeorgios/exercise9/blob/main/exercise_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install libraries"
      ],
      "metadata": {
        "id": "0BmE4iIkUsjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "cb4eVJfKUVTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zijCPAlUOux"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install box2d"
      ],
      "metadata": {
        "id": "zZictiIKUnK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install renderlab"
      ],
      "metadata": {
        "id": "rZZhmxE9UqO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic functions\n",
        "\n",
        "Every gymnasium environment has 3 basic functions: step, reset, and render.\n",
        "Step: updates an environment with actions.\n",
        "Reset: Resets the environment to an initial state.\n",
        "Render: Renders the environment."
      ],
      "metadata": {
        "id": "n3XEeB5SVIQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LunarLander-v3 enviroment\n",
        "##-Observation space\n",
        "It is an 8-dimensional vector with information about the current state of the lunar lander. It has 1.The coordinates of x, 2.The coordinates of y, 3.The linear velocity of x, 4.The linear velocity of y, 5.its angle, 6. its angular velocity, 7.Boolean that represents if a the left leg is in contact with the ground or not, 8.Boolean that represents if a the right leg is in contact with the ground or not\n",
        "##-Action space\n",
        "It tells the lunar lander what to do. It has four discrete actions. 1.Do nothing, 2.Fire left orientation engine, 3.Fire main engine,4.Fire right orientation engine\n",
        "##-Reward Function\n",
        "After every step a reward is given."
      ],
      "metadata": {
        "id": "JGYbFsJ4N5vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a random agent and visualize its gameplay"
      ],
      "metadata": {
        "id": "-oy6ylu8QETr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import renderlab as rl\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "env = rl.RenderFrame(env, './output')\n",
        "\n",
        "osbservation,info = env.reset()\n",
        "while True:\n",
        "  action = env.action_space.sample()\n",
        "  osbservation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  if terminated or truncated:\n",
        "    break\n",
        "\n",
        "env.play()\n"
      ],
      "metadata": {
        "id": "yWGW2jzAVTyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the random agent 5 times and get its mean reward"
      ],
      "metadata": {
        "id": "F-rF5k6EQc_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "rewards = []\n",
        "osbservation,info = env.reset()\n",
        "k=5\n",
        "for i in range(k):\n",
        "  action = env.action_space.sample()\n",
        "  osbservation, reward, terminated, truncated, info = env.step(action)\n",
        "  rewards.append(reward)\n",
        "  print(f\"Reward for episode {i}: {reward}\")\n",
        "  if terminated or truncated:\n",
        "    continue\n",
        "\n",
        "print(f\"Mean reward: {np.mean(rewards)}\")\n"
      ],
      "metadata": {
        "id": "1ty192o2ZR84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install stable-baselines3"
      ],
      "metadata": {
        "id": "xf0IoYgKQobO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "toV9jXvUasFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use the DQN algorithm"
      ],
      "metadata": {
        "id": "BtEHP5cpQ0Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "import time\n",
        "\n",
        "dqn_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "dqn_model = DQN(\"MlpPolicy\", dqn_env, verbose=1)\n",
        "dqn_model.learn(total_timesteps=20000, log_interval=4, progress_bar=True)\n",
        "dqn_time = time.time() - start\n",
        "\n"
      ],
      "metadata": {
        "id": "g0VdpL3TagoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = dqn_env.reset()\n",
        "    while True:\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, dqn_reward, terminated, truncated, info = dqn_env.step(action)\n",
        "      total_reward += dqn_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        dqn_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(dqn_rewards)}\")\n"
      ],
      "metadata": {
        "id": "qdCcglqob4E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use the PPO algorithm"
      ],
      "metadata": {
        "id": "H-a0x8ezSZg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "ppo_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "ppo_model = PPO(\"MlpPolicy\", ppo_env, verbose=1)\n",
        "ppo_model.learn(total_timesteps=20000, progress_bar=True)\n",
        "ppo_time = time.time() - start"
      ],
      "metadata": {
        "id": "VS61v7FBwkiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = ppo_env.reset()\n",
        "    while True:\n",
        "      action, _states = ppo_model.predict(obs)\n",
        "      obs, ppo_reward, terminated, truncated, info = ppo_env.step(action)\n",
        "      total_reward += ppo_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        ppo_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(ppo_rewards)}\")"
      ],
      "metadata": {
        "id": "KFQwibryoP-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot the time it took to train for each algorithm"
      ],
      "metadata": {
        "id": "bKNqaXd7S2EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar([\"DQN\", \"PPO\"], [dqn_time, ppo_time])\n",
        "plt.title(\"Training time\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3CMOWK5kTJMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot rewards per episode for each algorithm"
      ],
      "metadata": {
        "id": "ecoa2GeBTsU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.title(\"Rewards per episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_AugCYvTUhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redoing the previous steps but with a different parameters for the algorithms"
      ],
      "metadata": {
        "id": "yVxEOuJnT-i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dqn_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "dqn_model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    dqn_env,\n",
        "    buffer_size=200_000,\n",
        "    learning_starts=10_000,\n",
        "    batch_size=128,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    target_update_interval=1_000,\n",
        ")\n",
        "dqn_model.learn(total_timesteps=500000, log_interval=400, progress_bar=True)\n",
        "dqn_time = time.time() - start\n",
        "\n",
        "dqn_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = dqn_env.reset()\n",
        "    while True:\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, dqn_reward, terminated, truncated, info = dqn_env.step(action)\n",
        "      total_reward += dqn_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        dqn_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(dqn_rewards)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kFnaJ7BgUE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "start = time.time()\n",
        "ppo_model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    ppo_env,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    gamma=0.99,\n",
        ")\n",
        "ppo_model.learn(total_timesteps=200000, progress_bar=True)\n",
        "ppo_time = time.time() - start\n",
        "\n",
        "ppo_rewards = []\n",
        "#k = 5\n",
        "for i in range(k):\n",
        "    total_reward = 0\n",
        "    obs, info = ppo_env.reset()\n",
        "    while True:\n",
        "      action, _states = ppo_model.predict(obs)\n",
        "      obs, ppo_reward, terminated, truncated, info = ppo_env.step(action)\n",
        "      total_reward += ppo_reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        ppo_rewards.append(total_reward)\n",
        "        print(f\"Reward for episode {i}: {total_reward}\")\n",
        "        break\n",
        "\n",
        "print(f\"Mean reward: {np.mean(ppo_rewards)}\")"
      ],
      "metadata": {
        "id": "_sp4VRB7UJOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar([\"DQN\", \"PPO\"], [dqn_time, ppo_time])\n",
        "plt.title(\"Training time\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yeST8TLZUUlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dqn_rewards, label=\"DQN\")\n",
        "plt.plot(ppo_rewards, label=\"PPO\")\n",
        "plt.title(\"Rewards per episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dAeCIMPkUWDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}